{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckv49uWpCrWN"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZ8ESjQdlopn"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import pprint\n",
    "import requests\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from io import BytesIO\n",
    "from IPython.display import YouTubeVideo\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGa2Ilw6wqIg"
   },
   "source": [
    "## History\n",
    "\n",
    "Object detection shares a similar history to image recognition.\n",
    "They both currently use convolutional neural networks and both require large image datasets.\n",
    "With convolutional neural networks, they share the common origina of the Neocognitron, LeNet, and AlexNet, but object detection was inspired by different methods that were used before the current era of neural networks.\n",
    "\n",
    "There were two popular methods in the 2000s:\n",
    "\n",
    "\n",
    "*   Lowe's scale-invariant feature transform (SIFT) method\n",
    "*   Dalel and Trigg's historgram of oriented gradients (HOG)\n",
    "\n",
    "They both use some type of convolutional features: a Gaussian is used in SIFT and a Gaussian or a Sobel operator are used in HOG.\n",
    "\n",
    "The current methods are primarily convolutional neural network based.\n",
    "They can be roughly classified into two types: single stage and two stage.\n",
    "A single stage detector predicts an object and a bounding box for that object.\n",
    "Two stage object detection extracts a region proposal (potential bounding boxes), then classifies these regions, e.g. does it have a car, a dog, a person, etc.\n",
    "\n",
    "We'll be focusing on the single stage classifiers, in particular, the YOLO (You Only Look Once) class of models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X31JoEE9uLRT"
   },
   "source": [
    "# Lakota AI Code Camp Lesson 03: Introduction to Object Detection\n",
    "\n",
    "Object detection is a subfield of computer vision.\n",
    "It is focused on detecting if an object is in an image and if it is in the image, where it's located.\n",
    "Let's look at an example.\n",
    "\n",
    "The following is the label of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvpM5n-vsEMg"
   },
   "outputs": [],
   "source": [
    "object_dict = {\n",
    "   \"objects\":[\n",
    "      {\n",
    "         \"rectangle\":{\n",
    "            \"x\":730,\n",
    "            \"y\":66,\n",
    "            \"w\":135,\n",
    "            \"h\":85\n",
    "         },\n",
    "         \"object\":\"kitchen appliance\",\n",
    "         \"confidence\":0.501\n",
    "      },\n",
    "      {\n",
    "         \"rectangle\":{\n",
    "            \"x\":523,\n",
    "            \"y\":377,\n",
    "            \"w\":185,\n",
    "            \"h\":46\n",
    "         },\n",
    "         \"object\":\"computer keyboard\",\n",
    "         \"confidence\":0.51\n",
    "      },\n",
    "      {\n",
    "         \"rectangle\":{\n",
    "            \"x\":471,\n",
    "            \"y\":218,\n",
    "            \"w\":289,\n",
    "            \"h\":226\n",
    "         },\n",
    "         \"object\":\"Laptop\",\n",
    "         \"confidence\":0.85,\n",
    "         \"parent\":{\n",
    "            \"object\":\"computer\",\n",
    "            \"confidence\":0.851\n",
    "         }\n",
    "      },\n",
    "      {\n",
    "         \"rectangle\":{\n",
    "            \"x\":654,\n",
    "            \"y\":0,\n",
    "            \"w\":584,\n",
    "            \"h\":473\n",
    "         },\n",
    "         \"object\":\"person\",\n",
    "         \"confidence\":0.855\n",
    "      }\n",
    "   ],\n",
    "   \"requestId\":\"25018882-a494-4e64-8196-f627a35c1135\",\n",
    "   \"metadata\":{\n",
    "      \"height\":473,\n",
    "      \"width\":1260,\n",
    "      \"format\":\"Jpeg\"\n",
    "   },\n",
    "   \"modelVersion\":\"2021-05-01\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha26KXcLsLfL"
   },
   "source": [
    "We are downloading an image from a website, then we are converting it into it an array of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiQ8XHtksOnN"
   },
   "outputs": [],
   "source": [
    "img_src = \"https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/images/windows-kitchen.jpg\"\n",
    "\n",
    "resp = requests.get(img_src)\n",
    "\n",
    "img = Image.open(BytesIO(resp.content))\n",
    "\n",
    "img_array = np.asarray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DRNJtH0sqy7"
   },
   "source": [
    "Images are typically stored in a 3 channel format.\n",
    "In our case, the channels are red, green, and blue.\n",
    "It is a combination of the different intensities of red, green, and blue in each matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "0zcb5bklsfwJ",
    "outputId": "a88c2d36-f99b-4afc-e4ad-11a5794b20f3"
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "SfANZub_sgcW",
    "outputId": "0448637d-3abd-40a3-a716-2c7ec9d3a720"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(473 / 30, 1260 / 30))\n",
    "plt.imshow(img_array[:,:,0], cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "h1JcO5mOtaFW",
    "outputId": "5a7586f1-2b7b-400e-acb3-e5a7dc392e7e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(473 / 30, 1260 / 30))\n",
    "plt.imshow(img_array[:,:,1], cmap='Greens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "O_D7sE-4tcNE",
    "outputId": "ab4511b9-ef2c-43c4-db42-e3de71b4bc67"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(473 / 30, 1260 / 30))\n",
    "plt.imshow(img_array[:,:,2], cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e0aQGedugU4"
   },
   "source": [
    "Now, we display the label of the image on the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "iKnOh1Uytd5u",
    "outputId": "d6f94712-1575-42d9-8162-c54680eb3b93"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(473 / 30, 1260 / 30))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(img)\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = []\n",
    "for t in object_dict['objects']:\n",
    "    rectangle = t['rectangle']\n",
    "    x = rectangle['x']\n",
    "    y = rectangle['y']\n",
    "    w = rectangle['w']\n",
    "    h = rectangle['h']\n",
    "    obj=t['object']\n",
    "    rect.append((x, y, w, h, obj))\n",
    "\n",
    "# Add the patch to the Axes\n",
    "for r in rect:\n",
    "    x, y, w, h = r[:4]\n",
    "\n",
    "    ax.add_patch(\n",
    "        patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    )\n",
    "    plt.annotate(r[-1], (x, y), color='green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxnK73fRu7Jy"
   },
   "outputs": [],
   "source": [
    "sobel_x = torch.Tensor([[1, 0, -1],\n",
    "                        [2, 0, -2],\n",
    "                        [1, 0, -1]])\n",
    "\n",
    "sobel_y = torch.Tensor([[1, 2, 1],\n",
    "                        [0, 0, 0],\n",
    "                        [-1, -2, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzyT1_XV_9ug"
   },
   "outputs": [],
   "source": [
    "sobel_x = torch.cat((sobel_x, sobel_x, sobel_x)).reshape(3, 3, 3)\n",
    "\n",
    "sobel_y = torch.cat((sobel_y, sobel_y, sobel_y)).reshape(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCVxVjfjAXkh"
   },
   "outputs": [],
   "source": [
    "sobel_x = torch.cat((sobel_x, sobel_x, sobel_x)).reshape(3, 3, 3, 3)\n",
    "\n",
    "sobel_y = torch.cat((sobel_y, sobel_y, sobel_y)).reshape(3, 3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = torch.Tensor(img_array.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0P-St02-Frp"
   },
   "outputs": [],
   "source": [
    "out_img_x = F.conv2d(torch.permute(img_tensor, (2, 0, 1)).unsqueeze(0), weight=sobel_x)\n",
    "out_img_y = F.conv2d(torch.permute(img_tensor, (2, 0, 1)).unsqueeze(0), weight=sobel_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hc9vrrCbESNX"
   },
   "outputs": [],
   "source": [
    "out_img = torch.sqrt(torch.square(out_img_x) + torch.square(out_img_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRc6heDtEs0q"
   },
   "outputs": [],
   "source": [
    "out_img /= torch.max(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdcG__2VA1FC"
   },
   "outputs": [],
   "source": [
    "out_img = out_img.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6an1zagDBIMw"
   },
   "outputs": [],
   "source": [
    "out_img = torch.permute(out_img, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "Yxs92-ow_Lup",
    "outputId": "4a46ae22-0aaf-4f7c-f858-959b498aecf7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(473 / 30, 1260 / 30))\n",
    "plt.imshow(out_img.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do8a7BiHG6-H"
   },
   "source": [
    "## Datasets\n",
    "\n",
    "We're going to quickly go over some of the datasets.\n",
    "We're going to go over them in more detail later.\n",
    "\n",
    "There are 4 datasets that arose out of the need for high quality annotated images to train neural networks:\n",
    "\n",
    "\n",
    "1.   Caltech 101\n",
    "1.   Pascal VOC\n",
    "1.   MS COCO\n",
    "1.   Objects 365.\n",
    "\n",
    "Caltech 101 originated in 2003 by several researchers, Fei-Fei Li, Marco Andreetto, Marc'Aurelio Ranzato, and Pietro Perona, at the California Institute of Technology. Let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufDX61oh-633",
    "outputId": "b91f60ea-7ab5-4fa3-eb4f-5e754e4658e6"
   },
   "outputs": [],
   "source": [
    "caltech101_ds = torchvision.datasets.Caltech101(root='../data', target_type=['category','annotation'], download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdP1Qinq8cUy",
    "outputId": "af1593b1-6ce2-4937-d546-87a01e7cddc9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "caltech101_ds.annotation_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garfield = caltech101_ds.annotation_categories[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caltech101_ds.annotation_categories[caltech101_ds[0][1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GT7mdcG1z7Rg"
   },
   "outputs": [],
   "source": [
    "def show_rand_image():\n",
    "    num = torch.randint(low=0, high=8677, size=(1, )).item()\n",
    "\n",
    "    image, label = caltech101_ds[num]\n",
    "    \n",
    "    category, annotation = label\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.plot(annotation[0, :], annotation[1, :])\n",
    "    plt.title(caltech101_ds.annotation_categories[category])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "ZKwYz78WHzce",
    "outputId": "b40e60df-0756-4748-b0a6-9786e024ca39"
   },
   "outputs": [],
   "source": [
    "show_rand_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garfield_indices = [\n",
    "    5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120,\n",
    "    5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129,\n",
    "    5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138,\n",
    "    5139, 5140, 5141, 5142, 5143, 5144, 5145\n",
    "]\n",
    "\n",
    "num = np.random.randint(low=0, high=len(garfield_indices))\n",
    "\n",
    "image, label = caltech101_ds[garfield_indices[num]]\n",
    "    \n",
    "category, annotation = label\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.plot(annotation[0, :], annotation[1, :])\n",
    "plt.title(caltech101_ds.annotation_categories[category])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSYPnsFy_rln"
   },
   "outputs": [],
   "source": [
    "root = '../data/caltech101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eE2JCIK_ywh"
   },
   "outputs": [],
   "source": [
    "categories = sorted(os.listdir(os.path.join(root, \"101_ObjectCategories\")))\n",
    "categories.remove(\"BACKGROUND_Google\")  # this is not a real class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNlIM4pjBB7x"
   },
   "outputs": [],
   "source": [
    "name_map = {\n",
    "            \"Faces\": \"Faces_2\",\n",
    "            \"Faces_easy\": \"Faces_3\",\n",
    "            \"Motorbikes\": \"Motorbikes_16\",\n",
    "            \"airplanes\": \"Airplanes_Side_2\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTgwIuLSA-i2"
   },
   "outputs": [],
   "source": [
    "annotation_categories = list(map(lambda x: name_map[x] if x in name_map else x, categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oywoDOLUAGY2"
   },
   "outputs": [],
   "source": [
    "index = []\n",
    "y = []\n",
    "for (i, c) in enumerate(categories):\n",
    "    n = len(os.listdir(os.path.join(root, \"101_ObjectCategories\", c)))\n",
    "    index.extend(range(1, n + 1))\n",
    "    y.extend(n * [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_4jA-MrArVA"
   },
   "outputs": [],
   "source": [
    "target_type = ['annotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOSF4A1M2MhS"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "img = Image.open(\n",
    "    os.path.join(\n",
    "        root,\n",
    "        \"101_ObjectCategories\",\n",
    "        categories[y[num]],\n",
    "        f\"image_{index[num]:04d}.jpg\",\n",
    "    )\n",
    ")\n",
    "\n",
    "target = []\n",
    "for t in target_type:\n",
    "    if t == \"category\":\n",
    "        target.append(y[num])\n",
    "    elif t == \"annotation\":\n",
    "        data = scipy.io.loadmat(\n",
    "            os.path.join(\n",
    "                root,\n",
    "                \"Annotations\",\n",
    "                annotation_categories[y[num]],\n",
    "                f\"annotation_{index[num]:04d}.mat\",\n",
    "            )\n",
    "        )\n",
    "        target.append(data[\"obj_contour\"])\n",
    "target = tuple(target) if len(target) > 1 else target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrJHskta_qHX",
    "outputId": "9fcda1ec-02eb-4b01-a097-2f645972372a"
   },
   "outputs": [],
   "source": [
    "pp.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBPNyXnaDg8h"
   },
   "outputs": [],
   "source": [
    "y1, y2, x1, x2 = data['box_coord'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "7H8hJxUWBP0G",
    "outputId": "45123b29-e62c-4bfe-b70d-d8cc6131943e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(img)\n",
    "\n",
    "# Create a Rectangle patch\n",
    "# Add the patch to the Axes\n",
    "plt.plot(target[0, :] + x1, target[1, :] + y1)\n",
    "\n",
    "ax.add_patch(\n",
    "    patches.Rectangle([x1, y1], width=x2-x1, height=y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh3RPOIAEUSN"
   },
   "source": [
    "Let's look at Pascal VOC now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKQv-DhVC81t",
    "outputId": "866aa284-3aa1-4f4d-f7fe-0e42e9bd8fab"
   },
   "outputs": [],
   "source": [
    "pascal_voc_ds = torchvision.datasets.VOCDetection(root='../data/', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XirH6vYEfRV",
    "outputId": "2dbed90b-ece7-4473-f56b-5f5c0dd79b23"
   },
   "outputs": [],
   "source": [
    "num = torch.randint(low=0, high=len(pascal_voc_ds), size=(1, )).item()\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = pascal_voc_ds[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(target['annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "RmT4pwVmFHLj",
    "outputId": "ca19b2b8-ecc2-416a-8643-2bc1bdf567e0"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(image)\n",
    "\n",
    "annotations = target['annotation']['object']\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = []\n",
    "for t in annotations:\n",
    "    rectangle = t['bndbox']\n",
    "    xmin = int(rectangle['xmin'])\n",
    "    ymin = int(rectangle['ymin'])\n",
    "    xmax = int(rectangle['xmax'])\n",
    "    ymax = int(rectangle['ymax'])\n",
    "    x = xmin\n",
    "    y = ymin\n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    obj=t['name']\n",
    "    rect.append((x, y, w, h, obj))\n",
    "\n",
    "# Add the patch to the Axes\n",
    "for r in rect:\n",
    "    x, y, w, h = r[:4]\n",
    "\n",
    "    ax.add_patch(\n",
    "        patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    )\n",
    "    plt.annotate(r[-1], (x, y), color='green')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5MxSfSCudrL"
   },
   "source": [
    "We'll examine the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZhBhsHsFfXU",
    "outputId": "a18bef38-35cc-4596-9eec-7c63f247fc99"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/cocodataset/cocoapi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt6u1am_uH_2",
    "outputId": "dedfee4a-0d11-4225-9014-2402a56449d2"
   },
   "outputs": [],
   "source": [
    "coco_ds = torchvision.datasets.CocoDetection(root='../data/val2017/images',\n",
    "                                             annFile='../data/val2017/annotations/instances_val2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL9QF71AyjHm"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/val2017/annotations/instances_val2017.json', 'r') as f:\n",
    "    coco_cats = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KIzT9V0z2Ml"
   },
   "outputs": [],
   "source": [
    "categories = [0] * 91\n",
    "\n",
    "for c in coco_cats['categories']:\n",
    "    categories[c['id']] = c['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgdhGNwVwqwq"
   },
   "outputs": [],
   "source": [
    "num = np.random.randint(low=0, high=len(coco_ds))\n",
    "\n",
    "image, label = coco_ds[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "vk0D2UjXxaSw",
    "outputId": "1c1fcff2-4327-4085-e111-f49f27a15cbd"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 20))\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(image)\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = []\n",
    "for t in label:\n",
    "    rectangle = t['bbox']\n",
    "    x, y, w, h = rectangle\n",
    "    obj = t['category_id']\n",
    "    rect.append((x, y, w, h, obj))\n",
    "\n",
    "# Add the patch to the Axes\n",
    "for r in rect:\n",
    "    x, y, w, h = r[:4]\n",
    "\n",
    "    ax.add_patch(\n",
    "        patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    )\n",
    "    plt.annotate(categories[r[-1]], (x, y), color='green')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CMN4g5i07PI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
